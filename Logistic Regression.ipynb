{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#import packages for w2v\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('database.sqlite') \n",
    "\n",
    "#filtering only positive and negative reviews i.e. not taking into consideration those reviews with Score=3\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "\n",
    "print(filtered_data.shape) #looking at the number of attributes and size of the data\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "filtered_data[\"Time\"]=pd.to_datetime(filtered_data[\"Time\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "#Deduplication of entries\n",
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364173, 10)\n"
     ]
    }
   ],
   "source": [
    "#sording data by timestamp so that it can be divided in train and test dataset for time based slicing.\n",
    "final = final.sort_values('Time',axis=0,kind=\"quicksort\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18e09f88748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e7fde0f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "final=final.sample(2000, replace=True)\n",
    "print(final.shape)\n",
    "\n",
    "#To know how many positive and negative reviews are present in our dataset\n",
    "final['Score'].value_counts()\n",
    "final['Score'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stopwords, punctuation and then we do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"should've\", \"you'll\", 'until', \"that'll\", \"you've\", 'will', 'this', 'now', 'be', 'own', \"mightn't\", 'had', 'those', 'ma', 't', 'same', 'herself', 'wouldn', 'your', \"don't\", 'on', 'do', 'off', 'under', 'have', 'she', 'is', 'that', 'm', \"shouldn't\", 'yourself', \"she's\", 'again', 'them', \"hasn't\", 'i', 'are', 'did', 'was', 'where', \"doesn't\", 'here', 'hers', \"isn't\", 'couldn', 'for', 'in', 'but', 'y', 'its', 'itself', 'or', 'out', 'before', 'don', 'should', 'only', 'been', 'whom', \"needn't\", 'at', \"wasn't\", 'it', 'mustn', 'haven', \"you're\", 'myself', 'what', 'who', 'didn', 'of', 'an', 've', \"hadn't\", 'needn', 'a', 'isn', 'above', 'having', 'about', 'when', 'too', 'won', 'were', 'has', 'any', 'into', 'we', 'does', 'll', 'doesn', 'some', 's', 'him', \"aren't\", 'the', 'no', 'why', 'other', 'up', 'his', \"it's\", 'because', 'themselves', \"won't\", 'yourselves', 'doing', 'yours', 'as', 'than', 'most', 're', 'my', 'being', 'am', 'against', 'very', 'himself', 'if', 'they', 'weren', 'wasn', 'mightn', 'while', 'and', 'hasn', 'shan', 'just', 'both', 'd', 'me', 'over', 'o', \"couldn't\", 'from', 'our', 'more', 'through', 'their', \"you'd\", 'you', 'then', 'with', 'these', 'once', 'hadn', 'nor', \"weren't\", 'not', \"haven't\", 'theirs', 'there', 'shouldn', 'ourselves', \"didn't\", 'after', 'ain', 'how', 'he', 'between', \"wouldn't\", 'aren', 'below', 'down', 'ours', 'few', 'each', 'all', 'during', 'so', 'such', \"shan't\", \"mustn't\", 'by', 'can', 'to', 'which', 'further', 'her'}\n",
      "************************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "print(stop)\n",
    "print('************************************')\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 1: \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 0:\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161316    new newton fruit thin like tradtion fig newton...\n",
      "302124    shop tea ive learn brand name matter alway loo...\n",
      "238594    look like pretti decent coffe suitabl espresso...\n",
      "265877    great pair premium edg salmon sweet potato dog...\n",
      "297517    pleas combin bottl bottl stubb regular sauc ma...\n",
      "248678    expens fruitcak market expect least well made ...\n",
      "63095     browni point great custom servic browni truli ...\n",
      "29951     great tast crunch kid enjoy doesnt get star bi...\n",
      "89447     thrill could get rees creami peanut butter liv...\n",
      "85585     barbara peanut butter puffin addict care crunc...\n",
      "342808    youv use ghee butter eat strict paleo eat regi...\n",
      "113384    women listen tea tea mani good properti simpli...\n",
      "151215    excit amazon final stock perfect tear smaller ...\n",
      "361682    review say hard use room temperatur hard rock ...\n",
      "69611     everyth flavor pretzel crisp snack factori fla...\n",
      "176842    item list palet water plus howev receiv case w...\n",
      "86730     auto ship terrif way get hard find dog food li...\n",
      "181298    product tast similiar kix gluten free daughter...\n",
      "280638    love use coffe creamer well plain greek yogurt...\n",
      "119201    dont often way write review tri free sampl bar...\n",
      "113755    eat erewhon crispi brown rice day day month wi...\n",
      "254857    seedl appear week give littl bit patienc itll ...\n",
      "64500     like savori tast beef heart wont disappoint pr...\n",
      "253420    muffin good low calori muffin use whole egg in...\n",
      "24403     first would like say hand haribo make best gum...\n",
      "121676    sweetleaf stevia use anymor cant beat price ge...\n",
      "95907     seller ship quick great bargain compar superma...\n",
      "5389      coffe busi order various whole bean coffe amaz...\n",
      "349569    excit buy tri first time intern flight thought...\n",
      "196604    great calori count need measur guess portion s...\n",
      "                                ...                        \n",
      "209982    tri water day hype hype forget dont wast money...\n",
      "163485    first celesti season make great tea use combin...\n",
      "357479    coffe coffe husband drink move west coast get ...\n",
      "98071              rich tast tea excel hot tea look caffein\n",
      "324598    realli impress new line shampoo condition clea...\n",
      "125874    wow short shot amaz perfect spici great size t...\n",
      "47071     find excel qualiti almond reson price realli g...\n",
      "194888    small swedish fish arriv good condit fresh yum...\n",
      "17239     soy milk tast good drink toddler drink found w...\n",
      "121216    first introduc luxardo gourmet cherri eiffel t...\n",
      "346660    cant say whether true health promot effect exc...\n",
      "60145     buy son avers fresh fruit veg like made real f...\n",
      "173836    pleas order eat green appl everi day add point...\n",
      "347840    free item review drink lot herbal tea like kee...\n",
      "275167    last time ate chocol ago cant forget delici ta...\n",
      "186389    stuff smell tast delici flavor last make big c...\n",
      "128338    look cracker bring holiday gather could munch ...\n",
      "131663    like american ive never regular hot tea drinke...\n",
      "152459    love cracker light crisp doesnt lot calori don...\n",
      "292470    order singl box amazon receiv today sit drink ...\n",
      "362349    agre negat review item rubberi extrem salti ha...\n",
      "282547    use product everi spring fall tree shrub flour...\n",
      "34209     concern recd product made china number stick b...\n",
      "53413     love fruit decid purchas product dont wast mon...\n",
      "231562    love chocol bar glad find amazon com arriv pro...\n",
      "361610    starbuck fan sumatra favorit among dark roast ...\n",
      "12996     mislead list bottl contain one percent oil sus...\n",
      "282058    basket made three granddaught happi wonder arr...\n",
      "176497    great tast tea mellow flavor without aftertast...\n",
      "363271            thing like crack dog sure doggi sure love\n",
      "Name: CleanedText, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n",
    "print(final['CleanedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(3) #below the processed review can be seen in the CleanedText Column \n",
    "\n",
    "# store final table into an SQlLite table for future.\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews', conn,  schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_matrix_plot(y_test,pred):\n",
    "    labels = ['negative','positive']\n",
    "    cm = confusion_matrix(y_test, pred, labels)\n",
    "    print(cm)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    #fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and saving the data into respective files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final['CleanedText']\n",
    "y = final['Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\nfilename1 = 'train_data.sav'\\npickle.dump(X_train, open(filename1, 'wb'))\\n\\nfilename2 = 'test_data.sav'\\npickle.dump(X_test, open(filename2, 'wb'))\\n\\nX_train1 = pickle.load(open(filename1, 'rb'))\\nX_test1 = pickle.load(open(filename2, 'rb'))\\nimport math\\n\\nX_train =  X[:math.ceil(len(final)*.7)] \\nX_test = X[math.ceil(len(final)*.7):]\\ny_train = y[:math.ceil(len(final)*.7)]\\ny_test =  y[math.ceil(len(final)*.7):]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data to disk\n",
    "\"\"\"import pickle\n",
    "filename1 = 'train_data.sav'\n",
    "pickle.dump(X_train, open(filename1, 'wb'))\n",
    "\n",
    "filename2 = 'test_data.sav'\n",
    "pickle.dump(X_test, open(filename2, 'wb'))\n",
    "\n",
    "X_train1 = pickle.load(open(filename1, 'rb'))\n",
    "X_test1 = pickle.load(open(filename2, 'rb'))\n",
    "import math\n",
    "\n",
    "X_train =  X[:math.ceil(len(final)*.7)] \n",
    "X_test = X[math.ceil(len(final)*.7):]\n",
    "y_train = y[:math.ceil(len(final)*.7)]\n",
    "y_test =  y[math.ceil(len(final)*.7):]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2)) \n",
    "final_bow_count = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 46143)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec= count_vect.transform(X_test)\n",
    "X_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\n\\nscalar=StandardScaler()\\n#X, y = digits.data, digits.target\\nfinal_bow_count = scalar.fit_transform(final_bow_count)\\nX_test_vec = scalar.transform(X_test_vec)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar=StandardScaler()\n",
    "#X, y = digits.data, digits.target\n",
    "final_bow_count = scalar.fit_transform(final_bow_count)\n",
    "X_test_vec = scalar.transform(X_test_vec)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1 = 'bow_train_data.sav'\n",
    "pickle.dump([final_bow_count,y_train], open(f1, 'wb'))\n",
    "\n",
    "f2 = 'test_data.sav'\n",
    "pickle.dump([X_test_vec,y_test], open(f2, 'wb'))\n",
    "\n",
    "bow_train = pickle.load(open(f1, 'rb'))\n",
    "bow_test = pickle.load(open(f2, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing best hyperparameters (C, penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import uniform\n",
    "\n",
    "def grid_optimal_C(final_bow_count, y_train): \n",
    "    #final_bow_count, y_train\n",
    "    penalty = ['l1', 'l2']\n",
    "    C= [10**-4, 10**-2, 10**0, 10**2, 10**4]\n",
    "    \n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(final_bow_count, y_train, test_size=0.3)\n",
    "    \n",
    "\n",
    "    #Using GridSearchCV\n",
    "    model = GridSearchCV(LogisticRegression(), hyperparameters, scoring = 'f1', cv=5, n_jobs=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    opt_val=model.best_estimator_.get_params()['C']\n",
    "    best_penalty=model.best_estimator_.get_params()['penalty']\n",
    "    \n",
    "    return opt_val,best_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import random\n",
    "def randomsearch_optimal_C(final_bow_count, y_train):\n",
    "    # Create regularization penalty space\n",
    "    penalty = ['l1', 'l2']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(final_bow_count, y_train, test_size=0.3)\n",
    "\n",
    "    # Create regularization hyperparameter distribution using uniform distribution\n",
    "    C = uniform(loc=0, scale=1)\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    clf = RandomizedSearchCV(LogisticRegression(), hyperparameters, random_state=1, n_iter=1, cv=5, verbose=0, n_jobs=-1)\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    # View best hyperparameters\n",
    "    print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "    # Predict target vector\n",
    "    opt_val=best_model.best_estimator_.get_params()['C']\n",
    "    best_penalty=best_model.best_estimator_.get_params()['penalty']\n",
    "    \n",
    "    return opt_val,best_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Coefficient of each feature: [[0. 0. 0. ... 0. 0. 0.]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "\n",
    "opt_C,best_penalty = grid_optimal_C(bow_train[0],bow_train[1])\n",
    "lr = LogisticRegression(C=opt_C, penalty=best_penalty);\n",
    "lr.fit(bow_train[0],bow_train[1]);\n",
    "pred = lr.predict(bow_test[0])\n",
    "# y_test = count_vect.fit_transform(y_test)\n",
    "acc = accuracy_score(bow_test[1], pred) * 100\n",
    "print(opt_C)\n",
    "before_peturbation=lr.coef_\n",
    "print('Coefficient of each feature:', lr.coef_)\n",
    "print('Training accuracy:', lr.score(bow_train[0],bow_train[1]))\n",
    "print('Test accuracy:', lr.score(bow_test[0],bow_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 0.417022004702574\n",
      "10000\n",
      "Coefficient of each feature: [[0. 0. 0. ... 0. 0. 0.]]\n",
      "Training accuracy: 0.9107142857142857\n",
      "Test accuracy: 0.8816666666666667\n"
     ]
    }
   ],
   "source": [
    "opt,best_penalty=randomsearch_optimal_C(bow_train[0],bow_train[1])\n",
    "lr = LogisticRegression(C=opt, penalty=best_penalty);\n",
    "lr.fit(bow_train[0],bow_train[1]);\n",
    "pred = lr.predict(bow_test[0])\n",
    "# y_test = count_vect.fit_transform(y_test)\n",
    "acc = accuracy_score(bow_test[1], pred) * 100\n",
    "print(opt_C)\n",
    "print('Coefficient of each feature:', lr.coef_)\n",
    "print('Training accuracy:', lr.score(bow_train[0],bow_train[1]))\n",
    "print('Test accuracy:', lr.score(bow_test[0],bow_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 regularization : Reprting Sparsity and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01\n",
      "Sparsity with L1 penalty: 93.47%\n",
      "score with L1 penalty: 1.0000\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 93.55%\n",
      "score with L1 penalty: 1.0000\n",
      "C=100.00\n",
      "Sparsity with L1 penalty: 93.42%\n",
      "score with L1 penalty: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set regularization parameter\n",
    "for i, C in enumerate((0.01, 1, 100)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=opt_C, penalty='l1')\n",
    "    clf_l1_LR.fit(bow_train[0],bow_train[1])\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    #print(coef_l1_LR)\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(bow_train[0],bow_train[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Best Penalty: l1\n",
      "Best C: 0.417022004702574\n",
      "10000\n",
      "Before_peturbation [[0. 0. 0. ... 0. 0. 0.]]\n",
      "After_peturbation [[0. 0. 0. ... 0. 0. 0.]]\n",
      "Differences in coefficients [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = random.randint(1,10)\n",
    "print(i)\n",
    "f=bow_train[0]\n",
    "x = np.array([i])\n",
    "noise1=np.broadcast_to(x, (f.shape[0],f.shape[1]))\n",
    "#print(noise1)\n",
    "final_bow_count=final_bow_count+noise1\n",
    "opt,best_penalty=randomsearch_optimal_C(bow_train[0],bow_train[1])\n",
    "lr = LogisticRegression(C=opt, penalty=best_penalty);\n",
    "lr.fit(bow_train[0],bow_train[1]);\n",
    "pred = lr.predict(bow_test[0])\n",
    "acc = accuracy_score(bow_test[1], pred) * 100\n",
    "after_peturbation=lr.coef_\n",
    "print(opt_C)\n",
    "multi_collinearity=after_peturbation-before_peturbation\n",
    "print('Before_peturbation',before_peturbation)\n",
    "print('After_peturbation',after_peturbation)\n",
    "print('Differences in coefficients',multi_collinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e02ab5dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(after_peturbation)\n",
    "plt.ylabel('some numbers')\n",
    "plt.xlabel('coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_class_prob_sorted = lr.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = lr.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "#Sorting the features in the order of importance and printing the top 10 features\n",
    "print(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:10]))\n",
    "print(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing final results\n",
    "from astropy.table import Table, Column\n",
    "t = Table(names=( \"Hyper parameter\", \"Train error\",\"Test error\"), dtype=('i4', 'f4', 'f4'))\n",
    "t.add_row((optimal_alpha, train_error, test_error))\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
